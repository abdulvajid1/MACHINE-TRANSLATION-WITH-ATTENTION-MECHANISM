{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "3G1HJjntrcjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_size, hidden_size, vocab_size):\n",
        "    super().__init__(self)\n",
        "\n",
        "    # encoder initialization\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                             embedding_dim=embedding_size)\n",
        "    self.encoder_lstm = nn.LSTM(input_size=embedding_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           batch_first=True)\n",
        "\n",
        "  def forward(self,input):\n",
        "    # network flow\n",
        "    embedding_input = self.embedding(input)\n",
        "    encoder_outputs, (final_hidden_state, final_cell_state) = self.encoder_lstm(embedding_input)\n",
        "\n",
        "    return encoder_outputs, final_hidden_state, final_cell_state\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lPp3w84uZiHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size_tr,\n",
        "               embedding_dim,\n",
        "               hidden_size,\n",
        "               max_len=20,\n",
        "               sos_token=1,\n",
        "               ):\n",
        "\n",
        "    super().__init__()\n",
        "    self.MAX_LEN = max_len\n",
        "    self.SOS_TOKEN = sos_token\n",
        "\n",
        "    # Layers\n",
        "    self.embedding_layer = nn.Embedding(vocab_size_tr, embedding_dim)\n",
        "    self.lstm_layer = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.fnn = nn.Linear(hidden_size ,vocab_size_tr)\n",
        "    self.attention_vector = Attention(hidden_size)\n",
        "\n",
        "  def forward(self,\n",
        "              encoder_outputs,\n",
        "              hidden_state,\n",
        "              cell_state,\n",
        "              target_output=None):\n",
        "\n",
        "    # take the batch_size from encoder ouput since it got directly from trainloader that defines teh batchsize\n",
        "    batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "    decoder_input = torch.empty(size=(batch_size,1),dtype=torch.long).fill_(self.SOS_TOKEN)\n",
        "\n",
        "    # initializing the first token decoder input <start-token>\n",
        "    decoder_outputs = []\n",
        "\n",
        "    for i in range(self.MAX_LEN):\n",
        "      output_logits ,hidden_state = self.forward_step(encoder_outputs,decoder_input, hidden_state, cell_state)\n",
        "      decoder_outputs.append(output_logits.unsqueeze(1))\n",
        "      # decoder ouput = [(32,vocab_size),...(32,vocab_size)], this list will have max_len item , lastly we will concat this to make (32,max_len,vocab_size)\n",
        "\n",
        "      # teacher_forcing, occurs if we give target_output in the decoder\n",
        "      if target_output:\n",
        "        decoder_input = target_output[:,i].unsqueeze(1)\n",
        "      else:\n",
        "        _, decoder_input = output_logits.topk(1,dim=-1)\n",
        "\n",
        "    decoder_final_output = torch.cat(decoder_outputs,dim=1)\n",
        "\n",
        "    return decoder_final_output\n",
        "\n",
        "  def forward_step(self,encoder_outputs, decoder_input, hidden_state, cell_state):\n",
        "\n",
        "    embedded_decoder_input = self.embedding_layer(decoder_input)\n",
        "    # embedded shape : (32,1,embedd_size), here 1 , becuase we are giving each word or token to decoder and make it predict next word\n",
        "\n",
        "    lstm_output, (decoder_hidden, decoder_cell) = self.lstm_layer(embedded_decoder_input, (hidden_state, cell_state))\n",
        "    # lstm_output: (32,1,hidden_size)\n",
        "\n",
        "    output_logit = self.fnn(lstm_output.squeeze(1))\n",
        "    # squeeze (32,1,hidden_size) -> 32,hidden_size\n",
        "    # ouput_logits: (32,vocab_size) , 32 prediction of word , we will pic top item\n",
        "    hidden_state = self.attention_vector(encoder_outputs, decoder_hidden)\n",
        "\n",
        "    return output_logit ,hidden_state , cell_state"
      ],
      "metadata": {
        "id": "Q-0NVAH5Zq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super().__init__(self)\n",
        "\n",
        "    self.network = nn.Sequential(\n",
        "          nn.Linear(2*hidden_size,hidden_size),\n",
        "          nn.SELU(),\n",
        "          nn.Linear(hidden_size,1),\n",
        "          nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self,encoder_outputs,hidden_state):\n",
        "\n",
        "    # Concat encoeder_output and hidden_state, encoder_output shape = (32,timestept,hidden_size), hidden_state shape = 32,1,hidden_side\n",
        "    # first we need to make it same shape to concat hidden_state should be 32,timestep hidden_size, timestpe will be repeatation of same one vector from hidden size\n",
        "    encoder_timestep_len = encoder_outputs.size(1)\n",
        "\n",
        "    # hidden_size will be (1,32,hidden_size) according to doc we need to change\n",
        "    hidden_state = hidden_state.permute(1,0,2) # shape: (32,1,5)\n",
        "\n",
        "    # hidden_state repetation\n",
        "    hidden_repeated = hidden_state(1,encoder_timestep_len,1)\n",
        "\n",
        "    # concat with encoder_output\n",
        "    encoder_hidden_concat = torch.concat((encoder_outputs,hidden_repeated),dim=-1) # shape : 32,timestep,hidden_size*2\n",
        "\n",
        "    weights = self.network(encoder_hidden_concat) # (32,timestepe,1)\n",
        "\n",
        "    # for bmm\n",
        "    weights = weights.permute(0,2,1) # (32,1,timesteps)\n",
        "\n",
        "    context_vectores = weights.bmm(encoder_outputs) # 32,1,hidden_size\n",
        "\n",
        "    # convert back to the way lstm take hidden state\n",
        "    context_vectores = context_vectores.permute(1,0,2)\n",
        "\n",
        "    return context_vectores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V8hbPJZL1IKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqAttentionModel(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_size,\n",
        "               hidden_size,\n",
        "               vocab_size_en,\n",
        "               vocab_size_tr):\n",
        "    super().__init__(self)\n",
        "\n",
        "    self.encoder = Encoder(embedding_size,\n",
        "                           hidden_size,\n",
        "                           vocab_size_en)\n",
        "\n",
        "    self.decoder = Decoder(embedding_size,\n",
        "                           hidden_size,\n",
        "                           vocab_size_tr,\n",
        "                           max_len=20,\n",
        "                           sos_token=1)\n",
        "\n",
        "    def forward(self,input,target_output):\n",
        "      encoder_outputs, encoder_hidden_state, encoder_cell_state = self.encoder(input)\n",
        "      decoder_output = self.decoder(encoder_outputs, encoder_hidden_state, encoder_cell_state, target_output)\n",
        "\n",
        "      return decoder_output\n"
      ],
      "metadata": {
        "id": "W6_hxStX4-K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_size, hidden_size, vocab_size):\n",
        "    super().__init__(self)\n",
        "\n",
        "    # encoder initialization\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                             embedding_dim=embedding_size)\n",
        "    self.encoder_lstm = nn.LSTM(input_size=embedding_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           batch_first=True)\n",
        "\n",
        "  def forward(self,input):\n",
        "    # network flow\n",
        "    embedding_input = self.embedding(input)\n",
        "    encoder_outputs, (final_hidden_state, final_cell_state) = self.encoder_lstm(embedding_input)\n",
        "\n",
        "    return encoder_outputs, final_hidden_state, final_cell_state\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size_tr,\n",
        "               embedding_dim,\n",
        "               hidden_size,\n",
        "               max_len=20,\n",
        "               sos_token=1,\n",
        "               ):\n",
        "\n",
        "    super().__init__()\n",
        "    self.MAX_LEN = max_len\n",
        "    self.SOS_TOKEN = sos_token\n",
        "\n",
        "    # Layers\n",
        "    self.embedding_layer = nn.Embedding(vocab_size_tr, embedding_dim)\n",
        "    self.lstm_layer = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.fnn = nn.Linear(hidden_size ,vocab_size_tr)\n",
        "    self.attention_vector = Attention(hidden_size)\n",
        "\n",
        "  def forward(self,\n",
        "              encoder_outputs,\n",
        "              hidden_state,\n",
        "              cell_state,\n",
        "              target_output=None):\n",
        "\n",
        "    # take the batch_size from encoder ouput since it got directly from trainloader that defines teh batchsize\n",
        "    batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "    decoder_input = torch.empty(size=(batch_size,1),dtype=torch.long).fill_(self.SOS_TOKEN)\n",
        "\n",
        "    # initializing the first token decoder input <start-token>\n",
        "    decoder_outputs = []\n",
        "\n",
        "    for i in range(self.MAX_LEN):\n",
        "      output_logits ,hidden_state = self.forward_step(encoder_outputs,decoder_input, hidden_state, cell_state)\n",
        "      decoder_outputs.append(output_logits.unsqueeze(1))\n",
        "      # decoder ouput = [(32,vocab_size),...(32,vocab_size)], this list will have max_len item , lastly we will concat this to make (32,max_len,vocab_size)\n",
        "\n",
        "      # teacher_forcing, occurs if we give target_output in the decoder\n",
        "      if target_output:\n",
        "        decoder_input = target_output[:,i].unsqueeze(1)\n",
        "      else:\n",
        "        _, decoder_input = output_logits.topk(1,dim=-1)\n",
        "\n",
        "    decoder_final_output = torch.cat(decoder_outputs,dim=1)\n",
        "\n",
        "    return decoder_final_output\n",
        "\n",
        "  def forward_step(self,encoder_outputs, decoder_input, hidden_state, cell_state):\n",
        "\n",
        "    embedded_decoder_input = self.embedding_layer(decoder_input)\n",
        "    # embedded shape : (32,1,embedd_size), here 1 , becuase we are giving each word or token to decoder and make it predict next word\n",
        "\n",
        "    lstm_output, (decoder_hidden, decoder_cell) = self.lstm_layer(embedded_decoder_input, (hidden_state, cell_state))\n",
        "    # lstm_output: (32,1,hidden_size)\n",
        "\n",
        "    output_logit = self.fnn(lstm_output.squeeze(1))\n",
        "    # squeeze (32,1,hidden_size) -> 32,hidden_size\n",
        "    # ouput_logits: (32,vocab_size) , 32 prediction of word , we will pic top item\n",
        "    hidden_state = self.attention_vector(encoder_outputs, decoder_hidden)\n",
        "\n",
        "    return output_logit ,hidden_state , cell_state\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super().__init__(self)\n",
        "\n",
        "    self.network = nn.Sequential(\n",
        "          nn.Linear(2*hidden_size,hidden_size),\n",
        "          nn.SELU(),\n",
        "          nn.Linear(hidden_size,1),\n",
        "          nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self,encoder_outputs,hidden_state):\n",
        "\n",
        "    # Concat encoeder_output and hidden_state, encoder_output shape = (32,timestept,hidden_size), hidden_state shape = 32,1,hidden_side\n",
        "    # first we need to make it same shape to concat hidden_state should be 32,timestep hidden_size, timestpe will be repeatation of same one vector from hidden size\n",
        "    encoder_timestep_len = encoder_outputs.size(1)\n",
        "\n",
        "    # hidden_size will be (1,32,hidden_size) according to doc we need to change\n",
        "    hidden_state = hidden_state.permute(1,0,2) # shape: (32,1,5)\n",
        "\n",
        "    # hidden_state repetation\n",
        "    hidden_repeated = hidden_state.repeat(1,encoder_timestep_len,1)\n",
        "\n",
        "    # concat with encoder_output\n",
        "    encoder_hidden_concat = torch.concat((encoder_outputs,hidden_repeated),dim=-1) # shape : 32,timestep,hidden_size*2\n",
        "\n",
        "    weights = self.network(encoder_hidden_concat) # (32,timestepe,1)\n",
        "\n",
        "    # for bmm\n",
        "    weights = weights.permute(0,2,1) # (32,1,timesteps)\n",
        "\n",
        "    context_vectores = weights.bmm(encoder_outputs) # 32,1,hidden_size\n",
        "\n",
        "    # convert back to the way lstm take hidden state\n",
        "    context_vectores = context_vectores.permute(1,0,2)\n",
        "\n",
        "    return context_vectores\n",
        "\n",
        "\n",
        "class Seq2SeqAttentionModel(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_size,\n",
        "               hidden_size,\n",
        "               vocab_size_en,\n",
        "               vocab_size_tr):\n",
        "    super().__init__(self)\n",
        "\n",
        "    self.encoder = Encoder(embedding_size,\n",
        "                           hidden_size,\n",
        "                           vocab_size_en)\n",
        "\n",
        "    self.decoder = Decoder(embedding_size,\n",
        "                           hidden_size,\n",
        "                           vocab_size_tr,\n",
        "                           max_len=20,\n",
        "                           sos_token=1)\n",
        "\n",
        "  def forward(self,input,target_output):\n",
        "      encoder_outputs, encoder_hidden_state, encoder_cell_state = self.encoder(input)\n",
        "      decoder_output = self.decoder(encoder_outputs, encoder_hidden_state, encoder_cell_state, target_output)\n",
        "\n",
        "      return decoder_output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jqGGBe6yJ3os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yOUytD1J7vl",
        "outputId": "c38a255f-2809-4a9f-a8b3-a59c43f2f448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import Multi30k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "R1EtemYgLCKm",
        "outputId": "3a37900a-deda-41b9-8a21-f148ccebb25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-a4ca5f3b4cd6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMulti30k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# the following import has to happen first in order to load the torchtext C++ library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_extension\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m_TEXT_BUCKET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://download.pytorch.org/models/text/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0m_init_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext C++ Extension is not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libtorchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m# This import is for initializing the methods registered via PyBind11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# This has to happen after the base library is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.11/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.data.\n",
        "\n",
        "print(torch.__version__)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSkundV_LFqq",
        "outputId": "ddf33cee-ba13-45f6-8183-6b4f1bfbcc4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mY2K9QMfLT1L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}